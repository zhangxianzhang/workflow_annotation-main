#! https://zhuanlan.zhihu.com/p/417422301
# workflow 源码解析 11 : http 02

项目源码 : https://github.com/sogou/workflow

更加详细的源码注释可看 : https://github.com/chanchann/workflow_annotation

我们先看最简单的http例子

https://github.com/chanchann/workflow_annotation/blob/main/demos/07_http/http_req.cc

## 如何发送请求并接收

### Communicator::request

```cpp
int Communicator::request(CommSession *session, CommTarget *target)
{
	...
	ret = this->request_idle_conn(session, target);
	while (ret < 0)
	{
		entry = this->launch_conn(session, target);
		if (entry)
		{
			session->conn = entry->conn;
			session->seq = entry->seq++;
			data.operation = PD_OP_CONNECT;
			data.fd = entry->sockfd;
			data.ssl = NULL;
			data.context = entry;
			timeout = session->connect_timeout();
			if (mpoller_add(&data, timeout, this->mpoller) >= 0)
				break;

			this->release_conn(entry);
		}
	}

}
```

### request_idle_conn

我们首先复用连接发送

每一个连接的结构是

```cpp

struct CommConnEntry
{
	struct list_head list;
	CommConnection *conn;
	long long seq;
	int sockfd;
#define CONN_STATE_CONNECTING	0
#define CONN_STATE_CONNECTED	1
#define CONN_STATE_RECEIVING	2
#define CONN_STATE_SUCCESS		3
#define CONN_STATE_IDLE			4
#define CONN_STATE_KEEPALIVE	5
#define CONN_STATE_CLOSING		6
#define CONN_STATE_ERROR		7
	int state;
	int error;
	int ref;
	struct iovec *write_iov;
	SSL *ssl;
	CommSession *session;
	CommTarget *target;
	CommService *service;
	mpoller_t *mpoller;
	/* Connection entry's mutex is for client session only. */
	pthread_mutex_t mutex;
};
```

```cpp
int Communicator::request_idle_conn(CommSession *session, CommTarget *target)
{
	struct CommConnEntry *entry;

	pthread_mutex_lock(&target->mutex);
	entry = this->get_idle_conn(target);
	pthread_mutex_unlock(&target->mutex);

	pthread_mutex_lock(&entry->mutex);
	entry->session = session;
	session->conn = entry->conn;
	session->seq = entry->seq++;
	session->out = session->message_out(); // 这里是 CommMessageOut *ComplexHttpTask::message_out()
	this->send_message(entry);
	pthread_mutex_unlock(&entry->mutex);
}
```

这里就是先找个可复用连接

```cpp
struct CommConnEntry *Communicator::get_idle_conn(CommTarget *target)
{
	struct CommConnEntry *entry;
	list_for_each(pos, &target->idle_list)
	{
		entry = list_entry(pos, struct CommConnEntry, list);
		if (mpoller_set_timeout(entry->sockfd, -1, this->mpoller) >= 0)
		{
			list_del(pos);
			return entry;
		}
	}
}
```

然后调用ComplexHttpTask::message_out(), 用于拼凑req请求，自动添加一些字段。

message_out获得的是往连接上要发的数据。

此处我们在 : https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/04_http_improve.md

已经详细分析过

然后我们就是发送数据`send_message`, 这个我们放到后面来看

## 无可复用的连接

```cpp
entry = this->launch_conn(session, target);

session->conn = entry->conn;
session->seq = entry->seq++;
data.operation = PD_OP_CONNECT;
data.fd = entry->sockfd;
data.ssl = NULL;
data.context = entry;
timeout = session->connect_timeout();
mpoller_add(&data, timeout, this->mpoller);
...
```

如果没有可以复用的连接，我们先去建立连接，然后把connect操作挂到epoll上面监听(异步connect)

```cpp
struct CommConnEntry *Communicator::launch_conn(CommSession *session,
												CommTarget *target)
{
	//1. connect 建立连接
	sockfd = this->nonblock_connect(target);

	entry = (struct CommConnEntry *)malloc(sizeof (struct CommConnEntry));

	pthread_mutex_init(&entry->mutex, NULL);

	//2. 创建新的CommConnection
	// 然后初始化entry
	entry->conn = target->new_connection(sockfd);
	entry->seq = 0;
	entry->mpoller = this->mpoller;
	entry->service = NULL;
	entry->target = target;
	entry->session = session;
	entry->ssl = NULL;
	entry->sockfd = sockfd;
	entry->state = CONN_STATE_CONNECTING;
	entry->ref = 1;
}
```

### connect建立连接

```cpp

int Communicator::nonblock_connect(CommTarget *target)
{
	// 创建cfd
	int sockfd = target->create_connect_fd();
	...
	// 设置非阻塞
	__set_fd_nonblock(sockfd)
	...
	// 然后调用connec连接
	if (connect(sockfd, target->addr, target->addrlen) >= 0 ||
		errno == EINPROGRESS)
	{
		return sockfd;
	}
	...
}
```

```cpp
virtual int create_connect_fd()
{
	return socket(this->addr->sa_family, SOCK_STREAM, 0);
}
```

### 创建新的CommConnection

```cpp
virtual CommConnection *new_connection(int connect_fd)
{
	return new CommConnection;
}
```

### 异步connect

然后我们poller检测出这个事件后

```cpp
// __poller_thread_routines 中调用 __poller_handle_connect(node, poller);

static void __poller_handle_connect(struct __poller_node *node,
									poller_t *poller)
{
	socklen_t len = sizeof (int);
	int error;

	if (getsockopt(node->data.fd, SOL_SOCKET, SO_ERROR, &error, &len) < 0)
		error = errno;

	if (__poller_remove_node(node, poller))
		return;
	...

	poller->cb((struct poller_result *)node, poller->ctx);
}
```

### poller cb

这个在`poller_create` 的是便设置好 `poller->cb = params->callback;`

```cpp
struct poller_params
{
	...
	void (*callback)(struct poller_result *, void *);  
	void *context;
};
```

poller一切结果都通过callback返回

callback的第二个参数void *是poller_params里的context。

poller_start时，poller内部会创建一个线程进行epoll操作。callback是在这个线程里调起的。

因此，callback是在同一个线程里串行执行的。

因为一个读操作会得到很多条SUCCESS消息，保序的callback是必须的。

传给callback的poller_result *，没有const修饰，这个result指针是需要用户free的。

这么做的目的是减少malloc和free次数，以及减少不必要的内存拷贝。

此外，callback里的poller_result结尾处，有6个指针的空间可以利用，相当于你其实得到这个的一个结构：

```cpp
struct poller_result_EXT
{
	int state;
	int error;
	struct poller_data data;
	char avail[6 * sizeof (void *)];  
};
```

一切都是为了减少内存分配释放。总之最重要的是记得要自行free。

而这个callback在此处传入

```cpp
int Communicator::create_poller(size_t poller_threads)
{
	struct poller_params params = {
		.max_open_files		=	65536,
		.create_message		=	Communicator::create_message,
		.partial_written	=	Communicator::partial_written,
		.callback			=	Communicator::callback,
		.context			=	this
	};
	...
	this->mpoller = mpoller_create(&params, poller_threads);
	...
}
```

注意此处的`context = Communicator`

```cpp
void Communicator::callback(struct poller_result *res, void *context)
{
	Communicator *comm = (Communicator *)context;
	msgqueue_put(res, comm->queue);
}
```

所以我们回过头看

```cpp
poller->cb((struct poller_result *)node, poller->ctx);
```

这个就是把node(res) 放入这个msg queue里

这里就是到了`handler_threads`中的生产者(put)-消费者(get)模式了

所以我们把这个结果加到msg queue里，等待消费(get)

### Communicator::handler_thread_routine

```cpp

void Communicator::handler_thread_routine(void *context)
{
	Communicator *comm = (Communicator *)context;
	struct poller_result *res;

	while ((res = (struct poller_result *)msgqueue_get(comm->queue)) != NULL)
	{
		switch (res->data.operation)
		{
		...
		case PD_OP_CONNECT:
		case PD_OP_SSL_CONNECT:
			comm->handle_connect_result(res);
			break;
		...
	}
}
```

### Communicator::handle_connect_result

于是来处理connect，发送entry，并把read放到epoll上间监听

```cpp
void Communicator::handle_connect_result(struct poller_result *res)
{
	struct CommConnEntry *entry = (struct CommConnEntry *)res->data.context;
	CommSession *session = entry->session;
	CommTarget *target = entry->target;

	session->out = session->message_out();

	ret = this->send_message(entry);

	res->data.operation = PD_OP_READ;
	res->data.message = NULL;
	timeout = session->first_timeout();
	if (timeout == 0)
		timeout = Communicator::first_timeout_recv(session);
	else
	{
		session->timeout = -1;
		session->begin_time.tv_nsec = -1;
	}
	...
	mpoller_add(&res->data, timeout, this->mpoller);
	...
}
```


### send_message

```cpp
int Communicator::send_message(struct CommConnEntry *entry)
{
	struct iovec vectors[ENCODE_IOV_MAX];
	struct iovec *end;
	int cnt;

	cnt = entry->session->out->encode(vectors, ENCODE_IOV_MAX);
	...
	end = vectors + cnt;
	if (!entry->ssl)
	{
		cnt = this->send_message_sync(vectors, cnt, entry);
		if (cnt <= 0)
			return cnt;
	}

	return this->send_message_async(end - cnt, cnt, entry);
}
```

注意，这里的`entry->session->out->encode(vectors, ENCODE_IOV_MAX);`中

协议，需要提供协议的序列化和反序列化方法encode

encode函数在消息被发送之前调用，每条消息只调用一次。

encode函数里，用户需要将消息序列化到一个vector数组，数组元素个数不超过max。目前max的值为8192。

结构体struct iovec定义在请参考系统调用readv和writev。

encode函数正确情况下的返回值在0到max之间，表示消息使用了多少个vector。

encode返回-1表示错误。返回-1时，需要置errno。如果返回值>max，将得到一个EOVERFLOW错误。错误都在callback里得到。

为了性能考虑vector里的iov_base指针指向的内容不会被复制。所以一般指向消息类的成员。

![message_out](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/message_out01.png?raw=true)

这里的encode是HttpMessage的实现。此处先从略，等到HTTP协议解析部分再详解，就是先把消息序列化。

## send_message_sync

那么我们开始发送消息, 如果数据量很小，直接同步方式发送就可

这里分为几部分

- 第一部分 :

就是writev写到`struct iovec vectors[]`中

```cpp
CommSession *session = entry->session;
CommService *service;
int timeout;
ssize_t n;
int i;

while (cnt > 0)
{
	// On success, readv() and preadv() return the number of bytes read; 
	// writev() and pwritev() return the number of bytes written. 
	// On error, -1 is returned, and errno is set appropriately.
	n = writev(entry->sockfd, vectors, cnt <= IOV_MAX ? cnt : IOV_MAX);
	if (n < 0)
		return errno == EAGAIN ? cnt : -1;

	for (i = 0; i < cnt; i++)
	{
		if ((size_t)n >= vectors[i].iov_len)
			n -= vectors[i].iov_len;
		else
		{
			vectors[i].iov_base = (char *)vectors[i].iov_base + n;
			vectors[i].iov_len -= n;
			break;
		}
	}

	vectors += i;
	cnt -= i;
}
```

- 第二部分

如果entry中有`CommService`, 

主要操作就是

1. entry->ref++

2. mpoller_set_timeout

3. service->listen_fd > 0

如果是服务端的话，加入service的alive_list

list_add_tail(&entry->list, &service->alive_list);

alive_list是service上的成员，保存该serivce上所有keep alive的连接。这个list唯一的作用是drain，就是当连接数达到上限时用于关掉比较久没有使用的连接，以及程序退出的时候关闭所有keep alive连接。

如果是客户端的话，那么就mpoller_del，直接发送就可(todo)

```cpp
service = entry->service;
if (service)
{
	__sync_add_and_fetch(&entry->ref, 1);
	timeout = session->keep_alive_timeout();
	switch (timeout)
	{
	default:
		mpoller_set_timeout(entry->sockfd, timeout, this->mpoller);
		pthread_mutex_lock(&service->mutex);
		if (service->listen_fd >= 0)
		{
			entry->state = CONN_STATE_KEEPALIVE;
			list_add_tail(&entry->list, &service->alive_list);
			entry = NULL;
		}

		pthread_mutex_unlock(&service->mutex);
		if (entry)
		{
	case 0:
			mpoller_del(entry->sockfd, this->mpoller);
			entry->state = CONN_STATE_CLOSING;
		}
	}
}
```

- 第三部分

没有service的时候

```cpp
else   // if(!service)
{
	if (entry->state == CONN_STATE_IDLE)
	{
		timeout = session->first_timeout();
		if (timeout == 0)
			timeout = Communicator::first_timeout_recv(session);
		else
		{
			session->timeout = -1;
			session->begin_time.tv_nsec = -1;
		}

		mpoller_set_timeout(entry->sockfd, timeout, this->mpoller);
	}

	entry->state = CONN_STATE_RECEIVING;
}

```

## 补充小知识 异步connect(非阻塞的 connect)

在 socket 是阻塞模式下 connect 函数会一直到有明确的结果才会返回（或连接成功或连接失败）

如果服务器地址“较远”，连接速度比较慢，connect 函数在连接过程中可能会导致程序阻塞在 connect 函数处好一会儿（如两三秒之久）

- 流程

1. 创建socket，并将 socket 设置成非阻塞模式；

2. 调用 connect 函数，此时无论 connect 函数是否连接成功会立即返回；如果返回 -1 并不一定表示连接出错，如果此时错误码是EINPROGRESS，则表示正在尝试连接；

3. 接着添加到epoll中，在指定的时间内(timeout)判断该 socket 是否可写，如果可写说明连接成功，反之则认为连接失败。

linux中，connect 之后，不仅要用 epoll 检测可写，还要检测此时 socket 是否出错，通过错误码来检测确定是否连接上，错误码为 0 表示连接上，反之为未连接上